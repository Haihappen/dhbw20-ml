{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lineare Regression\n",
    "\n",
    "Lineare Regressionsmodelle sind ein guter Ausgangspunkt für Regressionsaufgaben.\n",
    "\n",
    "Solche Modelle sind beliebt, weil sie sehr schnell angepasst werden können und sehr gut interpretierbar sind. \n",
    "\n",
    "Die einfachste Form eines linearen Regressionsmodells ist die Anpassung einer geraden Linie an Daten, wir werden nachher aber auch sehen, wie man kompliziertere Modelle anpassen kann.\n",
    "\n",
    "Wir beginnen mit den Standard-Importen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "import numpy as np\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (10,8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Einfache lineare Regression\n",
    "\n",
    "Eine Gerade ist durch die Gleichung\n",
    "\n",
    "$y = b + ax$\n",
    "\n",
    "gegeben, wobei $a$ die Steigung und $b$ als y-Achsenabschnitt bezeichnet wird.\n",
    "\n",
    "Generieren wir uns ein paar Daten, die um die Gerade $y = -5 + 2x$ gestreut sind:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "m = 100\n",
    "\n",
    "rng = np.random.default_rng(1)\n",
    "x = 10 * rng.random(m)\n",
    "y = 2 * x - 5 + rng.standard_normal(m)\n",
    "sns.scatterplot(x, y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir können Scikit-Learns `LinearRegression Estimator` verwenden, um die beste Gerade zu ermitteln:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "model = LinearRegression(fit_intercept=True)\n",
    "\n",
    "model.fit(x[:, np.newaxis], y)\n",
    "\n",
    "xfit = np.linspace(0, 10, 1000)\n",
    "yfit = model.predict(xfit[:, np.newaxis])\n",
    "\n",
    "sns.scatterplot(x, y)\n",
    "sns.lineplot(xfit, yfit);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Steigung und der Intercept der Daten sind in den Anpassungsparametern des\n",
    "Modells enthalten, die in Scikit-Learn immer durch einen abschließenden Unterstrich\n",
    "gekennzeichnet sind. Hier sind die relevanten Parameter `coef_` und `intercept_`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Modell: y = {} + {} * x\".format(model.intercept_, model.coef_[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Wir sehen, dass die Ergebnisse sehr nahe an den Inputs liegen.\n",
    "\n",
    "Der LinearRegression-Schätzer ist jedoch noch viel leistungsfähiger.\n",
    "Er kann neben einfachen Geradengleichungen auch mehrdimensionale lineare Modelle\n",
    "der Form\n",
    "\n",
    "$y=a_0+a_1x_1+a_2x_2+\\dots$\n",
    "\n",
    "mit höher-dimensionalen x ermitteln.\n",
    "\n",
    "Geometrisch entspricht dies der Anpassung einer Ebene an Punkte in drei Dimensionen\n",
    "oder der Anpassung einer Hyperebene an Punkte in höheren Dimensionen.\n",
    "\n",
    "Die mehrdimensionale Natur solcher Regressionen macht es schwieriger, sie zu\n",
    "visualisieren, aber wir können eine dieser Anpassungen in Aktion sehen, indem wir\n",
    "einige Beispieldaten unter Verwendung des Matrixmultiplikationsoperators von NumPy\n",
    "erstellen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(1)\n",
    "X = rng.random((4, 3))\n",
    "y = 0.5 + np.dot(X, [1.5, -2., 1.])\n",
    "\n",
    "model.fit(X, y)\n",
    "print(model.intercept_)\n",
    "print(model.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Hier werden die $y$-Daten aus 4 zufälligen 3-dimensionalen $x$-Werten konstruiert, und die lineare\n",
    "Regression gewinnt die Koeffizienten zurück, die zur Konstruktion der Daten verwendet wurden.\n",
    "\n",
    "Auf diese Weise können wir den einzelnen LinearRegression-Schätzer verwenden, um\n",
    "Linien, Ebenen oder Hyperebenen an unsere Daten anzupassen. Es scheint immer noch,\n",
    "dass dieser Ansatz auf streng lineare Beziehungen zwischen Variablen beschränkt wäre,\n",
    "aber es stellt sich heraus, dass wir auch dies lockern können."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Basisfunktion Regression\n",
    "\n",
    "Ein Trick, mit dem Sie die lineare Regression an nicht-lineare Beziehungen zwischen\n",
    "Variablen anpassen können, besteht darin, die Daten entsprechend den Basisfunktionen\n",
    "zu transformieren.\n",
    "\n",
    "Die Idee besteht darin, unser mehrdimensionales lineares Modell zu nehmen\n",
    "\n",
    "$y=a_0+a_1x_1+a_2x_2+a_3x_3+\\dots$\n",
    "\n",
    "und die $x_1, x_2, x_3,$ usw. aus unserer jetzt wieder ein-dimensionalen Eingabe $x$ aufzubauen.\n",
    "Dazu lassen wir\n",
    "\n",
    "$x_n = f_n(x)$\n",
    "\n",
    "sein, wobei $f_n()$ eine Funktion ist, die unsere Daten transformiert.\n",
    "\n",
    "Wenn zum Beispiel $f_n(x) = x^n$ ist, wird unser Modell zu einer Polynom-Regression:\n",
    "\n",
    "$y=a_0+a_1x+a_2x^2+a_3x^3+\\dots$\n",
    "\n",
    "Beachten Sie, dass es sich nach wie vor um ein lineares Modell handelt - die\n",
    "Linearität bezieht sich auf die Tatsache, dass die Koeffizienten $a_n$ sich niemals\n",
    "miteinander multiplizieren oder dividieren. Was wir effektiv getan haben, ist,\n",
    "unsere eindimensionalen $x$-Werte in eine höhere Dimension zu projizieren, so\n",
    "dass eine lineare Anpassung kompliziertere Beziehungen zwischen $x$ und $y$\n",
    "einpassen kann.\n",
    "\n",
    "## Polynomische Basisfunktionen\n",
    "\n",
    "Diese Polynomprojektion ist so nützlich, dass sie mit Hilfe des\n",
    "`PolynomialFeatures-Transformators` in Scikit-Learn eingebaut ist:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "x = np.array([2, 3, 4])\n",
    "poly = PolynomialFeatures(3, include_bias=False)\n",
    "poly.fit_transform(x[:, None])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Wir sehen hier, dass der Transformator unser eindimensionales Array in ein\n",
    "dreidimensionales Array umgewandelt hat, indem er den Exponenten jedes Wertes\n",
    "genommen hat.\n",
    "\n",
    "Diese neue, höherdimensionale Datendarstellung kann dann in eine lineare Regression\n",
    "gesteckt werden. Der sauberste Weg, dies zu erreichen, ist die Verwendung einer sogenannten `Pipeline`.\n",
    "\n",
    "Erstellen wir auf diese Weise ein Polynom-Modell 7. Grades:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "poly_model = make_pipeline(PolynomialFeatures(7), LinearRegression())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Mit dieser Transformation können wir das lineare Modell verwenden, um viel\n",
    "kompliziertere Beziehungen zwischen $x$ und $y$ anzupassen. Hier ist zum Beispiel\n",
    "eine verrauschte Sinuswelle:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(1)\n",
    "x = 10 * rng.random(50)\n",
    "y = np.sin(x) + 0.1 * rng.standard_normal(50)\n",
    "\n",
    "poly_model.fit(x[:, np.newaxis], y)\n",
    "yfit = poly_model.predict(xfit[:, np.newaxis])\n",
    "plt.subplots(figsize=(9,6))\n",
    "plt.scatter(x, y)\n",
    "plt.plot(xfit, yfit);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Gaußsche Basisfunktionen\n",
    "\n",
    "Natürlich sind auch andere Basisfunktionen möglich. Ein nützliches Muster ist zum\n",
    "Beispiel die Anpassung eines Modells, das nicht eine Summe von Polynom-Basen,\n",
    "sondern eine Summe von Gaußschen Basen ist.\n",
    "Eine Gaußsche Basis sieht folgendermaßen aus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "x_all = np.arange(-10, 10, 0.001) # entire range of x, both in and out of spec\n",
    "# mean = 0, stddev = 1, since Z-transform was calculated\n",
    "y2 = norm.pdf(x_all,0,1)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(9,6))\n",
    "plt.style.use('fivethirtyeight')\n",
    "ax.plot(x_all,y2)\n",
    "ax.set_xlim([-4,4])\n",
    "ax.set_yticklabels([])\n",
    "ax.set_title('Normal Gaussian Curve')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    " Diese Gaußschen Basisfunktionen sind nicht in Scikit-Learn eingebaut, aber wir\n",
    " können einen benutzerdefinierten Transformator schreiben, der sie erzeugt.\n",
    "\n",
    " Dies ist in der folgenden Abbildung veranschaulicht.\n",
    "\n",
    " (Scikit-Learn-Transformatoren sind als Python-Klassen implementiert; der Source-Code\n",
    " ist eine gute Quelle, um zu sehen, wie sie erzeugt werden können)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import Gauss\n",
    "\n",
    "gauss_model = make_pipeline(Gauss.GaussianFeatures(20),\n",
    "                            LinearRegression())\n",
    "gauss_model.fit(x[:, np.newaxis], y)\n",
    "yfit = gauss_model.predict(xfit[:, np.newaxis])\n",
    "\n",
    "plt.scatter(x, y)\n",
    "plt.plot(xfit, yfit)\n",
    "plt.xlim(0, 10);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Wir stellen dieses Beispiel hier nur an, um deutlich zu machen, dass polynome\n",
    "Basisfunktionen nichts Magisches an sich haben: Wenn Sie eine Art Intuition in den\n",
    "Generierungsprozess Ihrer Daten haben, die Sie glauben lässt, dass die eine oder\n",
    "andere Basis angemessen sein könnte, können Sie sie ebenfalls verwenden.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
