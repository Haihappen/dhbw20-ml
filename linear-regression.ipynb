{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lineare Regression\n",
    "\n",
    "Lineare Regressionsmodelle sind ein guter Ausgangspunkt für Regressionsaufgaben.\n",
    "\n",
    "Solche Modelle sind beliebt, weil sie sehr schnell angepasst werden können und sehr gut interpretierbar sind. \n",
    "\n",
    "Die einfachste Form eines linearen Regressionsmodells ist die Anpassung einer geraden Linie an Daten, wir werden nachher aber auch sehen, wie man kompliziertere Modelle anpassen kann.\n",
    "\n",
    "Wir beginnen mit den Standard-Importen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "import numpy as np\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (10,8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Einfache lineare Regression\n",
    "\n",
    "Eine Gerade ist durch die Gleichung\n",
    "\n",
    "$y = b + ax$\n",
    "\n",
    "gegeben, wobei $a$ die Steigung der Gerade ist und $b$ als y-Achsenabschnitt bezeichnet wird.\n",
    "\n",
    "Generieren wir uns ein paar Daten, die um die Gerade $y = -5 + 2x$ gestreut sind:\n",
    "\n",
    "> NumPys `np.ramdom.default_rng` gibt einen Pseudo-Zufallsgenerator zurück. Damit wir bei unseren Experimenten\n",
    "> nachvollziehbare Werte bekommen, initiieren wir ihn mit einem festen `seed` - hier 1.\n",
    ">\n",
    "> `random` gibt Werte im Bereich $[0, 1)$ zurück. Mit `random(100)` erhalten wir ein Array von 100 Zufallswerten.\n",
    ">\n",
    "> `standard_normal` gibt analog Werte zurück, allerdings diesmal Standard-Normalverteilt. `random` ist gleichverteilt.\n",
    "\n",
    "**Beachte:** Obwohl `x` ein Array ist, können wir damit in normaler \"skalarer\" Schreibweise `y` als Array errechnen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "m = 100\n",
    "\n",
    "rng = np.random.default_rng(1)\n",
    "x = 10 * rng.random(m)\n",
    "y = 2 * x - 5 + rng.standard_normal(m)\n",
    "sns.scatterplot(x, y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir können Scikit-Learns `LinearRegression Estimator` verwenden, um die beste Gerade zu ermitteln:\n",
    "\n",
    "> `np.linspace` erzeugt ein \"linear verteiltes\" Array. D.h. `np.linspace(0, 10, 1000)` verteilt ermittelt 1000 gleichweit verteilte Punkte zwischen 0 und 10.\n",
    "\n",
    "> `np.newaxis` erhöht die Dimension eines Vektors oder einer Matrix: aus einer ein-dimensionalen Struktur wird eine zwei-Dimensionale etc.\n",
    "\n",
    "\n",
    "> `x[:,np.newaxis]` erzeugt aus dem Vektor (1D) $[x^{(1)}, x^{(2)}, \\dots, x^{(n)}]$ eine Matrix (2D) $[[x^{(1)}], [x^{(2)}], \\dots, [x^{(n)}]]$ - unsere Design-Matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = 10 * rng.random(5)\n",
    "\n",
    "print('Ursprüngliches Array v: {} - {}'.format(v.shape, v))\n",
    "print('Nach v[:, np.newaxis]: {} - \\n{}'.format(v[:, np.newaxis].shape, v[:, np.newaxis]))\n",
    "print('Nach v[np.newaxis, :]: {} - {}'.format(v[np.newaxis, :].shape, v[np.newaxis, :]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`variable.shape()` ist dein Freund: Es hilft regelmäßig zu prüfen, ob die Dimensionen der Matrizen tatsächlich zusammen passen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "model = LinearRegression(fit_intercept=True)\n",
    "\n",
    "model.fit(x[:, np.newaxis], y)\n",
    "\n",
    "xfit = np.linspace(0, 10, 1000)\n",
    "yfit = model.predict(xfit[:, np.newaxis])\n",
    "\n",
    "sns.scatterplot(x, y)\n",
    "sns.lineplot(xfit, yfit);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Steigung und der y-Achsenabschnitt sind in den Anpassungsparametern des\n",
    "Modells enthalten. Diese sind in Scikit-Learn immer durch einen abschließenden Unterstrich\n",
    "gekennzeichnet. Hier sind es die Parameter `coef_` und `intercept_`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Modell: y = {} + {} * x\".format(model.intercept_, model.coef_[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Wir sehen, dass die Ergebnisse sehr nahe an den Inputs liegen.\n",
    "\n",
    "Der LinearRegression-Schätzer ist jedoch noch viel leistungsfähiger.\n",
    "Er kann neben einfachen Geradengleichungen auch mehrdimensionale lineare Modelle\n",
    "der Form\n",
    "\n",
    "$y=a_0+a_1x_1+a_2x_2+\\dots$\n",
    "\n",
    "mit höher-dimensionalen x ermitteln.\n",
    "\n",
    "Geometrisch entspricht dies der Anpassung einer Ebene an Punkte in drei Dimensionen\n",
    "oder der Anpassung einer Hyperebene an Punkte in höheren Dimensionen.\n",
    "\n",
    "Die mehrdimensionale Natur solcher Regressionen macht es schwieriger, sie zu\n",
    "visualisieren, aber wir können ja einfach mal testen, ob die lineare Regression die Parameter für Eingabedaten\n",
    "**ohne Fehler** ermittelt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(1)\n",
    "X = rng.random((4, 3))\n",
    "y = 0.5 + np.dot(X, [1.5, -2., 1.])\n",
    "\n",
    "model.fit(X, y)\n",
    "print(model.intercept_)\n",
    "print(model.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Wir ermitteln $y$ als Punkte der Hyperebene, die durch die vier zufälligen Stützpunkte $x$ definiert sind, und die lineare\n",
    "Regression gewinnt die Koeffizienten zurück, die zur Konstruktion der Hyperebene verwendet wurden.\n",
    "\n",
    "> **Beachte:** Oben hatten wir Skalare mit einem Array multipliziert - dies können wir in Python mit dem gewöhnlichen Multiplikations-Operator tun. Hier multiplizieren wir zwei Vektoren: $(3,1)$ mal $(1,3)$ - dafür brauchen wir die [Matrizenmultiplikation (dot-Produkt)](https://de.wikipedia.org/wiki/Matrizenmultiplikation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Lineare Regression für nicht-lineare Daten?\n",
    "\n",
    "Was aber tun, wenn die Daten **nicht** durch ein lineares Modell abbildbar sind?\n",
    "\n",
    "Ein Trick, mit dem Sie die lineare Regression an nicht-lineare Beziehungen zwischen\n",
    "Variablen anpassen können, besteht darin, die Daten mit sogenannten **Basisfunktionen**\n",
    "zu transformieren.\n",
    "\n",
    "Die Idee besteht darin, unser mehrdimensionales lineares Modell zu nehmen\n",
    "\n",
    "$y=a_0+a_1x_1+a_2x_2+a_3x_3+\\dots$\n",
    "\n",
    "und mittels Basisfunktionen zu transformieren: Dazu lassen wir\n",
    "\n",
    "$x_i = f_i(x)$ für i=1...k\n",
    "\n",
    "sein, wobei $f_i$ eine Funktion ist, die unsere Daten transformiert.\n",
    "\n",
    "Wenn zum Beispiel $f_i(x) = x^i$ ist, wird unser Modell zu einer Polynom-Regression:\n",
    "\n",
    "$y=a_0+a_1x+a_2x^2+a_3x^3+\\dots$\n",
    "\n",
    "Beachten Sie, dass es sich nach wie vor um ein lineares Modell handelt - die\n",
    "Linearität bezieht sich auf die Tatsache, dass die Koeffizienten $a_i$ sich niemals\n",
    "miteinander multiplizieren oder dividieren. Was wir effektiv getan haben, ist,\n",
    "unsere $x$-Werte in eine höhere Dimension zu projizieren, so\n",
    "dass eine lineare Anpassung kompliziertere Beziehungen zwischen $x$ und $y$\n",
    "einpassen kann.\n",
    "\n",
    "## Polynomische Basisfunktionen\n",
    "\n",
    "Diese Polynomprojektion ist so nützlich, dass sie mit Hilfe des\n",
    "`PolynomialFeatures-Transformators` in Scikit-Learn eingebaut ist:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "x = np.array([2, 3, 4])\n",
    "poly = PolynomialFeatures(3, include_bias=False)\n",
    "poly.fit_transform(x[:, None])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Wir sehen hier, dass der Transformator unseren skalaren Wert in ein\n",
    "Array mit drei Elementen umgewandelt hat, indem er Quadrat und dritte Potenz unseres Wertes\n",
    "als \"Feature\" erzeugt hat.\n",
    "\n",
    "Diese neue, höherdimensionale Input-Darstellung kann dann in eine lineare Regression\n",
    "gesteckt werden. Der sauberste Weg, dies zu erreichen, ist die Verwendung einer sogenannten `Pipeline`.\n",
    "\n",
    "Erstellen wir auf diese Weise ein Polynom-Modell 7. Grades:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "poly_model = make_pipeline(PolynomialFeatures(7), LinearRegression())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Mit dieser Transformation können wir das lineare Modell verwenden, um viel\n",
    "kompliziertere Beziehungen zwischen $x$ und $y$ anzupassen. Hier ist zum Beispiel\n",
    "eine polynomiale Regression einer verrauschte Sinuswelle:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(1)\n",
    "x = 10 * rng.random(50)\n",
    "y = np.sin(x) + 0.1 * rng.standard_normal(50)\n",
    "\n",
    "poly_model.fit(x[:, np.newaxis], y)\n",
    "yfit = poly_model.predict(xfit[:, np.newaxis])\n",
    "sns.scatterplot(x, y)\n",
    "sns.lineplot(xfit, yfit);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Gaußsche Basisfunktionen\n",
    "\n",
    "Natürlich sind auch andere Basisfunktionen möglich. Ein nützliches Muster ist zum\n",
    "Beispiel die Anpassung eines Modells, das nicht eine Summe von Polynom-Basen,\n",
    "sondern eine Summe von Gaußschen Basen ist.\n",
    "Eine Gaußsche Basis sieht folgendermaßen aus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "x_all = np.arange(-10, 10, 0.001)\n",
    "y2 = norm.pdf(x_all,0,1)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(9,6))\n",
    "\n",
    "sns.lineplot(x_all,y2, ax=ax)\n",
    "ax.set_xlim([-4,4])\n",
    "ax.set_yticklabels([])\n",
    "ax.set_title('Gauß- / Normal-Verteilung');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    " Diese Gaußschen Basisfunktionen sind nicht in Scikit-Learn eingebaut, aber wir\n",
    " können einen benutzerdefinierten Transformator schreiben, der sie erzeugt.\n",
    "\n",
    " Hier ermöglichen wir also einen Fit der Daten mithilfe eine Summe von (im Beispiel 20) Gauß-Verteilungen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import Gauss\n",
    "\n",
    "gauss_model = make_pipeline(Gauss.GaussianFeatures(20),\n",
    "                            LinearRegression())\n",
    "gauss_model.fit(x[:, np.newaxis], y)\n",
    "yfit = gauss_model.predict(xfit[:, np.newaxis])\n",
    "\n",
    "sns.scatterplot(x, y)\n",
    "sns.lineplot(xfit, yfit);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Wir führen dieses Beispiel hier nur an, um deutlich zu machen, dass polynome\n",
    "Basisfunktionen nichts Magisches an sich haben: Wenn Sie eine Art Intuition in den\n",
    "Generierungsprozess Ihrer Daten haben, die Sie glauben lässt, dass die eine oder\n",
    "andere Basis angemessen sein könnte, können Sie sie ebenfalls verwenden.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
