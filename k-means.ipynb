{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# k-Means Algorithmus\n",
    "\n",
    "Die Aufgabe ist, gegebene Daten in möglichst ähnliche Cluster zu segmentieren.\n",
    "\n",
    "\"k\" bezieht sich dabei auf die Anzahl der Cluster, die man erhalten möchte."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Zunächst mal importieren wir ein paar Bibliotheken:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()  # for plot styling\n",
    "import numpy as np\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (10,8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Grobe Beschreibung\n",
    "\n",
    "Der k-Means-Algorithmus sucht nach einer vorgegebenen Anzahl von Clustern innerhalb eines unmarkierten mehrdimensionalen Datensatzes.\n",
    "Dazu verwendet er eine einfache Vorstellung davon, wie das optimale Clustering aussieht:\n",
    "\n",
    "-  Das \"Clusterzentrum\" ist das arithmetische Mittel aller zum Cluster gehörenden Punkte.\n",
    "-  Jeder Punkt liegt näher an seinem eigenen Clusterzentrum als an anderen Clusterzentren.\n",
    "\n",
    "Diese beiden Annahmen bilden die Grundlage des k-Means-Algorithmus. Wir werden bald genau darauf\n",
    "eingehen, wie der Algorithmus diese Lösung erreicht, aber sehen wir uns zunächst einen einfachen\n",
    "Datensatz an und sehen uns das Ergebnis des k-Means an."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Unsere Daten\n",
    "Zunächst erzeugen wir uns einen Datensatz:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "X, y_true = make_blobs(n_samples=300, centers=4,\n",
    "                       cluster_std=0.60, random_state=0)\n",
    "sns.scatterplot(X[:, 0], X[:, 1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Man sieht schon mit bloßem Auge vier Cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Fertiger Algorithmus in Scikit-Learn\n",
    "\n",
    "Wir nutzen nun den k-Means Algorithmus von Scikit-learn. Das Muster ist dabei immer gleich:\n",
    "\n",
    "- zunächst wählt man ein Modell\n",
    "- und ruft dann die `.fit`-Methode, um das Modell zu trainieren\n",
    "- schließlich erzeugt man mit der `.predict`-Methode die Ergebnisdaten\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=4)\n",
    "kmeans.fit(X)\n",
    "y_kmeans = kmeans.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Wir können das Ergebnis visualisieren - die großen roten Punkte zeigen die Zenter der Cluster an:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plt.scatter(X[:, 0], X[:, 1], c=y_kmeans, s=50, cmap='viridis')\n",
    "\n",
    "centers = kmeans.cluster_centers_\n",
    "plt.scatter(centers[:, 0], centers[:, 1], c='red', s=100);\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Implementierung\n",
    "\n",
    "Der Algorithmus ist einfach genug, dass wir ihn in wenigen Zeilen selbst schreiben können:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import pairwise_distances_argmin\n",
    "\n",
    "def find_clusters(X, n_clusters, rseed=2):\n",
    "    # 1. wähle zufällige Zentren:\n",
    "    rng = np.random.default_rng(rseed)\n",
    "    i = rng.permutation(X.shape[0])[:n_clusters]\n",
    "    centers = X[i]\n",
    "\n",
    "    while True:\n",
    "        # Färbe entsprechend dem nächtsgelegenen Zentrum ein\n",
    "        labels = pairwise_distances_argmin(X, centers)\n",
    "\n",
    "        # Ermittle neue Zentren als Mittelpunkt des Clusters\n",
    "        new_centers = np.array([X[labels == i].mean(0) for i in range(n_clusters)])\n",
    "\n",
    "        # Prüfe, ob fertig\n",
    "        if np.all(centers == new_centers):\n",
    "            break\n",
    "        centers = new_centers\n",
    "\n",
    "    return centers, labels\n",
    "\n",
    "centers, labels = find_clusters(X, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Schauen wir uns das Ergebnis an:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plt.scatter(X[:, 0], X[:, 1], c=labels, s=50, cmap='viridis')\n",
    "plt.scatter(centers[:, 0], centers[:, 1], c='red', s=100);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Wir können uns das auch interaktiv anschauen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from ipywidgets import interact\n",
    "from sklearn.metrics import pairwise_distances_argmin\n",
    "import ipywidgets as widgets\n",
    "\n",
    "def plot_kmeans_interactive(min_clusters=1, max_clusters=6):\n",
    "    def plot_points(X, labels, n_clusters):\n",
    "        plt.scatter(X[:, 0], X[:, 1], c=labels, s=50, cmap='viridis', vmin=0, vmax=n_clusters - 1)\n",
    "\n",
    "    def plot_centers(centers):\n",
    "        plt.scatter(centers[:, 0], centers[:, 1], marker='o',\n",
    "                    c=np.arange(centers.shape[0]),\n",
    "                    s=200, cmap='viridis')\n",
    "        plt.scatter(centers[:, 0], centers[:, 1], marker='o', c='red', s=100)\n",
    "\n",
    "    def _kmeans_step(frame=0, n_clusters=4, rseed=2):\n",
    "        labels = np.zeros(X.shape[0])\n",
    "        rng = np.random.default_rng(rseed)\n",
    "        i = rng.permutation(X.shape[0])[:n_clusters]\n",
    "        centers = X[i]\n",
    "\n",
    "        nsteps = frame // 3\n",
    "\n",
    "        for i in range(nsteps + 1):\n",
    "            old_centers = centers\n",
    "            if i < nsteps or frame % 3 > 0:\n",
    "                labels = pairwise_distances_argmin(X, centers)\n",
    "\n",
    "            if i < nsteps or frame % 3 > 1:\n",
    "                centers = np.array([X[labels == j].mean(0)\n",
    "                                    for j in range(n_clusters)])\n",
    "                nans = np.isnan(centers)\n",
    "                centers[nans] = old_centers[nans]\n",
    "\n",
    "        # plot the data and cluster centers\n",
    "        plot_points(X, labels, n_clusters)\n",
    "        plot_centers(old_centers)\n",
    "\n",
    "        # plot new centers if third frame\n",
    "        if frame % 3 == 2:\n",
    "            for i in range(n_clusters):\n",
    "                plt.annotate('', centers[i], old_centers[i],\n",
    "                             arrowprops=dict(arrowstyle='->', linewidth=3, color='yellow'))\n",
    "            plot_centers(centers)\n",
    "\n",
    "        plt.xlim(-4, 4)\n",
    "        plt.ylim(-2, 10)\n",
    "\n",
    "        if frame == 0:\n",
    "            plt.title(\"Setze die Zentren zufällig\")\n",
    "        elif frame % 3 == 1:\n",
    "            plt.title(\"1. Färbe die Punkte entsprechend dem nächsten Zentrum ein\")\n",
    "        elif frame % 3 == 2:\n",
    "            plt.title(\"2. Setze die Zentren auf den Cluster-Durchschnitt\")\n",
    "        elif frame % 3 == 0 and frame > 0:\n",
    "            plt.title(\"Neue Positionen der Zentren\")\n",
    "\n",
    "    return interact(_kmeans_step, frame=widgets.IntSlider(min=0, max=100, step=1, value=0),\n",
    "                    n_clusters=[1, 2, 3, 4, 5, 6, 15],\n",
    "                    rseed=[2,4])\n",
    "\n",
    "plot_kmeans_interactive();\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Limitierungen des k-Means Algorithmus\n",
    "\n",
    "### Die Anzahl der Cluster muss im Vorfeld gewählt werden\n",
    "\n",
    "Wie man sieht ist die Wahl von k ganz entscheidend für eine saubere Trennung der Cluster.\n",
    "\n",
    "- Zu kleines k differenziert die Cluster ggf. nicht ausreichend.\n",
    "- Zu großes k trennt ggf. willkürlich Cluster auf.\n",
    "\n",
    "### Lösung ist nicht unbedingt die global beste Lösung\n",
    "\n",
    "Das Verfahren konvergiert zwar garantiert, allerdings nicht notwendigerweise zur global optimalen Lösung.\n",
    "\n",
    "Hier wirkt sich ganz entscheidend die anfängliche Anordnung der Zentren aus. Daher wird die Berechnung\n",
    "in der Praxis mehrere Male mit jeweils zufällig gewählten Anfangszentren wiederholt.\n",
    "\n",
    "Scikit-Learn macht das defaultmäßig 10 mal, gesteuert über den Parameter `n_init`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "centers, labels = find_clusters(X, n_clusters=4, rseed=4)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=labels, s=50, cmap='viridis')\n",
    "plt.scatter(centers[:, 0], centers[:, 1], c='red', s=100, alpha=0.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Langsam, bei großen Datensets\n",
    "\n",
    "Da jede Iteration von k-Means auf jeden Punkt im Datensatz zugreifen muss, kann der Algorithmus\n",
    "relativ langsam sein, wenn die Anzahl der Stichproben wächst. Sie könnten sich fragen, ob diese\n",
    "Anforderung, alle Daten bei jeder Iteration zu verwenden, gelockert werden kann;\n",
    "beispielsweise könnten Sie einfach eine Teilmenge der Daten verwenden, um die Clusterzentren\n",
    "bei jedem Schritt zu aktualisieren.\n",
    "\n",
    "Dies ist die Idee hinter den batchbasierten k-Means-Algorithmen, von denen eine Form\n",
    "in `sklearn.cluster.MiniBatchKMeans` implementiert ist. Die Schnittstelle hierfür ist die\n",
    "gleiche wie für Standard-K-Means.\n",
    "\n",
    "### Limitiert auf lineare Cluster Grenzen\n",
    "\n",
    "Die Grenze zwischen zwei Clustern ist immer eine Gerade, daher hat der Algorithmus Probleme, wenn\n",
    "die Cluster kompliziertere Formen haben:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "X_moon, y = make_moons(200, noise=.05, random_state=0)\n",
    "kmeans = KMeans(2, random_state=0)\n",
    "kmeans.fit(X_moon)\n",
    "y_means = kmeans.predict(X_moon)\n",
    "centers = kmeans.cluster_centers_\n",
    "plt.scatter(X_moon[:, 0], X_moon[:, 1], c=y_means, s=50, cmap='viridis')\n",
    "plt.scatter(centers[:, 0], centers[:, 1], c='red', s=100);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Alternative Algorithmen\n",
    "\n",
    "- Gaussian mixture models (bessere Bewertung über Anzahl der Cluster)\n",
    "- Algorithmen, die Anzahl der Cluster selber wählen können:\n",
    "    - DBSCAN\n",
    "    - mean-shift\n",
    "    - affinity propagation"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
